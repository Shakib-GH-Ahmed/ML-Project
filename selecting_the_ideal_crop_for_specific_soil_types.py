# -*- coding: utf-8 -*-
"""Selecting the Ideal Crop for Specific Soil Types.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ob9haOFSYiFgFLRNIIJ-OvQ4KnhYuyyk
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score

"""# Read The Dataset"""

file_link = 'https://drive.google.com/file/d/1xkgz9ctaSnJmnXB_Ffc6OHig-rZcdthB/view?usp=sharing'
id = file_link.split("/")[-2]
dataset = f'https://drive.google.com/uc?id={id}'
data = pd.read_csv(dataset)

"""# Initial Observation"""

data.head()

"""Showing the unique class for label"""

unique_values = data['label'].unique()
print(unique_values)

"""Checking is there any null value exist or not"""

data.isnull().sum()

"""Dataset Shape"""

data.shape

"""Checking is there any Duplicate value or not"""

data.duplicated()

"""Describe Dataset"""

data.describe()

"""# Preprocessing

Replace the naming convention
"""

data.rename(columns={'N': 'Nitrogen', 'P': 'Phosphorus','K':'Potassium','label':'Crops'}, inplace=True)

"""After replace the name the Dataset look"""

data.head()

"""# Encoding"""

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

le

data['Crops']=le.fit_transform(data[['Crops']])

"""After Encoding the Dataset look"""

data.head()

"""Seeing the unique Class"""

unique_values = data['Crops'].unique()
print(unique_values)

"""Split the Dataset"""

x=data.drop('Crops',axis=1)

x

y=data[['Crops']]

y

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(x)

scaled_data

for col_name in x.columns:
    if x[col_name].nunique()>3:
        x[col_name]=scaler.fit_transform(x[[col_name]])

x

"""# Train Test Split"""

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=42)

"""

```
# This is formatted as code
```

# Model Function"""

def evaluation_of_classifier(clf, xtrain, ytrain, xtest, ytest, clf_name):
    clf.fit(xtrain, ytrain)
    ypred = clf.predict(xtest)
    print(f"Classification Report for {clf_name}:")
    print(classification_report(ytest, ypred))
    accuracy_val = accuracy_score(ytest, ypred)
    print(f'{clf_name} Validation Accuracy: {accuracy_val:.2%}')
    conf_matrix = confusion_matrix(ytest, ypred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title(f'Confusion Matrix for {clf_name}')
    plt.show()

"""# Logistic Regression Classifier"""

from sklearn.linear_model import LogisticRegression

logistic_r=LogisticRegression()

logistic_r

"""Evaluate the classifier with confusion matric, precision,recall and accuracy"""

evaluation_of_classifier(logistic_r, xtrain, ytrain, xtest, ytest, "Logistic Regression")



"""## **Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier

decision_t=DecisionTreeClassifier()

decision_t

"""Evaluate the classifier with confusion matric, precision,recall and accuracy"""

evaluation_of_classifier(decision_t, xtrain, ytrain, xtest, ytest, "Decision Tree")

"""# K nearest Neighbors Classifier"""

from sklearn.neighbors import KNeighborsClassifier

kneighbors=KNeighborsClassifier(n_neighbors= 5)

kneighbors

"""Evaluate the classifier with confusion matric, precision,recall and accuracy"""

evaluation_of_classifier(kneighbors, xtrain, ytrain, xtest, ytest, "K Nearest Neighbors")

"""# Naive Bayes Classifier"""

from sklearn.naive_bayes import GaussianNB

gNb=GaussianNB()

gNb

"""Evaluate the classifier with confusion matric, precision,recall and accuracy"""

evaluation_of_classifier(gNb, xtrain, ytrain, xtest, ytest, "Naive Bayes")

"""# Model Evaluation"""

import numpy as np

def evaluate_classifier_metrics(clf, xtrain, ytrain, xtest, ytest, clf_name):
    clf.fit(xtrain, ytrain)
    y_pred = clf.predict(xtest)
    accuracy_val = accuracy_score(ytest, y_pred)
    report = classification_report(ytest, y_pred, output_dict=True)
    precision = report['weighted avg']['precision']
    recall = report['weighted avg']['recall']
    f1 = report['weighted avg']['f1-score']

    return {
        'Classifier': clf_name,
        'Accuracy': accuracy_val,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }

"""Initialize Classifier"""

models= {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(),
    "KNN": KNeighborsClassifier()
}

"""Taking a list for store metrices data"""

metrics_data = []

"""
Evaluate classifiers and store metrics"""

for clf_name, clf in models.items():
    metrics = evaluate_classifier_metrics(clf, xtrain, ytrain, xtest, ytest, clf_name)
    metrics_data.append(metrics)

"""Convert metrices data into dataframe and plot"""

metrics_df = pd.DataFrame(metrics_data)
plt.figure(figsize=(50, 10))
colors = sns.color_palette('viridis', 4)

"""# Plotting Graph of Accuracy for all model"""

plt.figure(figsize=(10, 4))
plt.subplot(1, 1, 1)
sns.barplot(x='Classifier', y='Accuracy', data=metrics_df, palette=[colors[0]])
plt.title('Accuracy')
for index, value in enumerate(metrics_df['Accuracy']):
    plt.text(index, value, f'{value:.4f}', ha='center')
plt.tight_layout()
plt.show()

"""# Plotting Graph of Precision for all model"""

plt.figure(figsize=(10, 4))
plt.subplot(1,1,1)
sns.barplot(x='Classifier', y='Precision', data=metrics_df, palette=[colors[1]])
plt.title('Precision')
for index, value in enumerate(metrics_df['Precision']):
    plt.text(index, value, f'{value:.4f}', ha='center')
plt.tight_layout()
plt.show()

"""# Plotting Graph of Recall for all model"""

plt.figure(figsize=(10, 4))
plt.subplot(1,1,1)
sns.barplot(x='Classifier', y='Recall', data=metrics_df, palette=[colors[2]])
plt.title('Recall')
for index, value in enumerate(metrics_df['Recall']):
    plt.text(index, value, f'{value:.4f}', ha='center')
plt.tight_layout()
plt.show()

"""# Plotting Graph of F1 Score for all model"""

plt.figure(figsize=(10, 4))
plt.subplot(1,1,1)
sns.barplot(x='Classifier', y='F1 Score', data=metrics_df, palette=[colors[3]])
plt.title('F1 Score')
for index, value in enumerate(metrics_df['F1 Score']):
    plt.text(index, value, f'{value:.4f}', ha='center')

plt.tight_layout()
plt.show()

"""# Evaluation for Training and Testing Accuracy for all model

Prediction Function
"""

def train_predict(model, xtrain, ytrain, xtest):
    model.fit(xtrain, ytrain)
    train_predictions = model.predict(xtrain)
    test_predictions = model.predict(xtest)
    train_accuracy = accuracy_score(ytrain, train_predictions)
    test_accuracy = accuracy_score(ytest, test_predictions)
    return train_accuracy, test_accuracy

"""Ploting Accuracies"""

def plot_accuracies(models, train_accuracies, test_accuracies):
    x = np.arange(len(models))
    width = 0.35

    fig, ax = plt.subplots(figsize=(15, 6))
    bars1 = ax.bar(x - width/2, train_accuracies, width, label='Train Accuracy', color='orange')
    bars2 = ax.bar(x + width/2, test_accuracies, width, label='Test Accuracy', color='lightgreen')

    ax.set_xlabel('Model')
    ax.set_ylabel('Accuracy')
    ax.set_title('Prediction Accuracy on Training and Test Data for Different Models')
    ax.set_xticks(x)
    ax.set_xticklabels(models, rotation=45)
    ax.legend()

    def autolabel(bars):
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.4f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom')

    autolabel(bars1)
    autolabel(bars2)
    plt.tight_layout()
    plt.show()

models = [
    ("Logistic Regression", LogisticRegression()),
    ("KNN", KNeighborsClassifier()),
    ("Decision Tree", DecisionTreeClassifier()),
    ("Naive Bayes", GaussianNB())
]

train_accuracies = []
test_accuracies = []
for model_name, model in models:
    train_accuracy, test_accuracy = train_predict(model, xtrain, ytrain, xtest)
    train_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)
    print(f"{model_name}: Train Accuracy = {train_accuracy:.4f}, Test Accuracy = {test_accuracy:.4f}")
plot_accuracies([model_name for model_name, _ in models], train_accuracies, test_accuracies)

"""# Poject Evaluation

To accomplish this, multiple machine learning methods were used and evaluated for their accuracy in predicting crop suitability. Our study looks at the performance of four prominent classifiers: Logistic Regression, Naive Bayes, Decision Tree, and K-Nearest Neighbors (KNN). Naive Bayes achieved the highest accuracy of 99.39%, followed by Decision Tree at 98.64%.
KNN achieved an impressive 97.73% accuracy, while Logistic Regression achieved 94.55%. Among that, Logistic Regression had the lowest accuracy at 94.55%, indicating potential limitations in its application to this job. These findings highlight the effectiveness of ensemble approaches such as Naive Bayes and Descision tree for making precise crop recommendations based on soil characteristics, as well as the importance of selecting algorithms that are adapted to the complexity of agricultural datasets. Such insights can considerably impact agricultural decision-making processes, leading to increased crop output and resource utilization efficiency.
"""